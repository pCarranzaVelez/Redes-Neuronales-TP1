{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Open Files","metadata":{}},{"cell_type":"code","source":"PATH = '../input/rossman-data/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table_names = ['train', 'test', 'store', 'store_states', 'state_names', 'googletrend', 'weather']\ntrain, test, store, store_states, state_names, googletrend, weather = dfs = [pd.read_csv(PATH + fname+'.csv', \n                                                                             low_memory=False) for fname in table_names]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\nEl preprocesamiento se puede dividir en dos tareas principales. Una de ellas es la de llenar los vacios de información en donde falte. Esto principalmente se realiza a través de tomar valores default para aquellos casos donde no se tienen información. Ejemplos de valores default son minimos, maximos, promedios, etc; o a su vez datos conocidos en base a la naturaleza del problema, por ejemplo, si es un día habil entonces se encuentra abierto.\nLa otra tarea principal es de unificar los datos de la competencia con aquellos datos utiles externos. Esto es importante ya que se puede obtener mucha información relevante de fuentes externas que nos permiten obtener conclusiones en base a distintas correlaciones que se pueden observar. Por ejemplo, se podría observar el efecto del clima sobre la actividad de una sucursal. Es por eso que parte del preprocesamiento es conectar estos datos en un mismo dataframe.","metadata":{}},{"cell_type":"code","source":"import datetime\nfrom isoweek import Week","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Me fijo en que lugares falta información\ndef get_missing_columns(df):\n    return list(df.columns[df.describe(include = 'all').loc['count']<len(df)])\n\nfor i, df in enumerate(dfs):\n    print(table_names[i], get_missing_columns(df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Si no es dia 7 esta abierto - Verificado que el store 622 esta usualmente cerrado el día 7\ntest.loc[test['Open'].isna(), 'Open'] = (test[test['Open'].isna()]['DayOfWeek'] != 7)*1.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fechas de las mas antiguas segun datos y tambien las fechas anteriores las pongo como minimo en 1990\nstore.loc[store['CompetitionOpenSinceYear'].isna() | (store['CompetitionOpenSinceYear']<1990), 'CompetitionOpenSinceYear'] = np.int32(1990)\nstore.loc[store['CompetitionOpenSinceMonth'].isna() | (store['CompetitionOpenSinceYear']<1990), 'CompetitionOpenSinceMonth'] = np.int32(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Si no tengo la distancia, supongo que no hay (Como si estuviera lejos)\nprint(store['CompetitionDistance'].max(), store['CompetitionDistance'].mean(), store['CompetitionDistance'].min())\nstore.loc[store['CompetitionDistance'].isna(), 'CompetitionDistance'] = store['CompetitionDistance'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Si no tiene Promo2 entonces la semana en donde empezo es cero. Esto será tratado como categórica por eso tiene sentido\nstore.loc[store['Promo2SinceWeek'].isna() & (store['Promo2'] == 0), 'Promo2SinceWeek'] = np.int32(1)\nstore.loc[store['Promo2SinceYear'].isna() & (store['Promo2'] == 0), 'Promo2SinceYear'] = np.int32(1990)\nstore.loc[store['PromoInterval'].isna() & (store['Promo2'] == 0), 'PromoInterval'] = '-'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Supongo que son dias tranquilos (de todas formas no se usaran para la predicción)\nweather.loc[weather['Max_VisibilityKm'].isna(), 'Max_VisibilityKm'] = weather['Max_VisibilityKm'].max()\nweather.loc[weather['Mean_VisibilityKm'].isna(), 'Mean_VisibilityKm'] = weather['Mean_VisibilityKm'].max()\nweather.loc[weather['Min_VisibilitykM'].isna(), 'Min_VisibilitykM'] = weather['Min_VisibilitykM'].max()\nweather.loc[weather['Max_Gust_SpeedKm_h'].isna(), 'Max_Gust_SpeedKm_h'] = weather['Max_Gust_SpeedKm_h'].min()\nweather.loc[weather['CloudCover'].isna(), 'CloudCover'] = weather['CloudCover'].min()\nweather.loc[weather['Events'].isna(), 'Events'] = 'Sunny'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Para confirmar si aun falta información\nfor i, df in enumerate(dfs):\n    print(table_names[i], get_missing_columns(df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Para simplificar el análisis, se generan nuevas columnas que permiten obtener datos importantes, como la cantidad de\n# días respecto a una fecha. Para ello, primero se toman como número las fechas para poder realizar las funciones adecuadas.\nstore['CompetitionOpenSinceYear'] = store['CompetitionOpenSinceYear'].astype(np.int32)\nstore['CompetitionOpenSinceMonth'] = store['CompetitionOpenSinceMonth'].astype(np.int32)\nstore['Promo2SinceWeek'] = store['Promo2SinceWeek'].astype(np.int32)\nstore['Promo2SinceYear'] = store['Promo2SinceYear'].astype(np.int32)\n\n# Se crean nuevas columnas con fecha de apertura en formato de fecha (dia 15 por que no tengo el dato y no es relevante)\nstore['CompetitionOpenSince'] = pd.to_datetime(store.apply(lambda x: datetime.datetime(\n    x.CompetitionOpenSinceYear, x.CompetitionOpenSinceMonth, 15), axis=1))\n\nstore[\"Promo2Since\"] = pd.to_datetime(store.apply(lambda x: Week(\n    x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataFrame Joining\nA su vez, ya que se tienen datos fuera de aquellos dados por la competencia, ellos se deben asociar a aquellos dados por la competencia para poder utilizar las conexiones provenientes. Esto se realiza principalmente porque datos como el clima o el googletrend pueden ser relevantes en una ubicación y fecha especifica. Entonces, se deben unir los datos en base a la ubicación y fecha.","metadata":{}},{"cell_type":"code","source":"# Esta función permite unir dos dataframes en uno solo\ndef join_df(left, right, left_on, right_on=None):\n    if right_on is None: right_on = left_on\n    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n                      suffixes=(\"\", \"_y\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unir al clima y los nombres de los estados\n","metadata":{}},{"cell_type":"code","source":"weather = join_df(weather, state_names, \"file\", \"StateName\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocesar los datos de googletrend para que los nombres esten coherentes al resto de los datos ","metadata":{}},{"cell_type":"code","source":"googletrend.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Armo columan date con el primer día de la semana\ngoogletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"googletrend.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Armo columna de State con el nomnre del estado\ngoogletrend['State'] = googletrend.file.str.split('_', expand=True)[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"googletrend.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notar que un estado tiene un acrónimo diferente\nstate_names['State']\n# Lo corrijo\ngoogletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ordeno todas las fechas para que esten con el mismo formato","metadata":{}},{"cell_type":"code","source":"def add_datepart(df):\n    df.Date = pd.to_datetime(df.Date)\n    df[\"Year\"] = df.Date.dt.year\n    df[\"Month\"] = df.Date.dt.month\n    df[\"Week\"] = df.Date.dt.week\n    df[\"Day\"] = df.Date.dt.day\n    \nadd_datepart(weather)\nadd_datepart(googletrend)\nadd_datepart(train)\nadd_datepart(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trends generales de Alemania tienen el state en None\ntrend_de = googletrend[googletrend.file == 'Rossmann_DE']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unir todos los datos para que esten en un unico train y test","metadata":{}},{"cell_type":"code","source":"# Agrego acronimo de state al store\nstore = join_df(store, store_states, \"Store\")\n\n# Mergeo train y store\njoined_train = join_df(train, store, \"Store\")\n\n# Mergeo test y store\njoined_test = join_df(test, store, \"Store\")\n\n# Mergeo con googletrend\njoined_train = join_df(joined_train, googletrend, [\"State\",\"Year\", \"Week\"])\njoined_train = joined_train.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n\njoined_test = join_df(joined_test, googletrend, [\"State\",\"Year\", \"Week\"])\njoined_test = joined_test.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n\njoined_train = join_df(joined_train, weather, [\"State\",\"Date\"])\njoined_test = join_df(joined_test, weather, [\"State\",\"Date\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Obtengo el número de días desde la apertura de una competencia y de una promoción","metadata":{}},{"cell_type":"code","source":"# Competencia\njoined_train[\"CompetitionDaysOpen\"] = joined_train.Date.subtract(joined_train[\"CompetitionOpenSince\"]).dt.days\njoined_test[\"CompetitionDaysOpen\"] = joined_test.Date.subtract(joined_test[\"CompetitionOpenSince\"]).dt.days\n\n# Corrige errores de la formula anterior\njoined_train.loc[joined_train.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\njoined_train.loc[joined_train.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0\n\njoined_test.loc[joined_test.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\njoined_test.loc[joined_test.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0\n\n# Lo pone en meses y limita a 2 años como máximo\njoined_train[\"CompetitionMonthsOpen\"] = joined_train[\"CompetitionDaysOpen\"]//30\njoined_train.loc[joined_train.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\n\njoined_test[\"CompetitionMonthsOpen\"] = joined_test[\"CompetitionDaysOpen\"]//30\njoined_test.loc[joined_test.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\n\n# Promoción\njoined_train[\"Promo2Days\"] = joined_train.Date.subtract(joined_train[\"Promo2Since\"]).dt.days\njoined_test[\"Promo2Days\"] = joined_test.Date.subtract(joined_test[\"Promo2Since\"]).dt.days\n\n# Corrige errores de la formula anterior\njoined_train.loc[joined_train.Promo2Days<0, \"Promo2Days\"] = 0\njoined_train.loc[joined_train.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n\njoined_test.loc[joined_test.Promo2Days<0, \"Promo2Days\"] = 0\njoined_test.loc[joined_test.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n\n# Lo pone en semanas y limita a 25 semanas como máximo\njoined_train[\"Promo2Weeks\"] = joined_train[\"Promo2Days\"]//7\njoined_train.loc[joined_train.Promo2Weeks<0, \"Promo2Weeks\"] = 0\njoined_train.loc[joined_train.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n\njoined_test[\"Promo2Weeks\"] = joined_test[\"Promo2Days\"]//7\njoined_test.loc[joined_test.Promo2Weeks<0, \"Promo2Weeks\"] = 0\njoined_test.loc[joined_test.Promo2Weeks>25, \"Promo2Weeks\"] = 25","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sacar duplicados","metadata":{}},{"cell_type":"code","source":"joined_train = joined_train.loc[:,~joined_train.columns.duplicated()]\njoined_test = joined_test.loc[:,~joined_test.columns.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convierto los dataframes en .fth","metadata":{}},{"cell_type":"code","source":"%time joined_train.to_feather('joined_train_before_durations.fth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time joined_test.to_feather('joined_test_before_durations.fth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## StateHoliday en Boolean\nPara simplificar, conviene dividir los feriados en solo dos casos, que los haya o que no los haya.","metadata":{}},{"cell_type":"code","source":"joined_train['StateHoliday_bool'] = joined_train.StateHoliday!='0'\njoined_test['StateHoliday_bool'] = joined_test.StateHoliday!='0'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Duración de Eventos\nEs común trabajar con los datos de tiempo en filas en vez de columnas para simplificar la obtención de promedios o duraciones de eventos asociados a estos datos temporales.\nPara obtener esta duración de eventos, se desarrolla la clase elapsed que permite obtener y ordenar los datos.\nA su vez, determinar duraciones entre eventos o las demandas antes o despues de cierto evento nos dá la posibilidad de observar patrones de demanda cerca, o durante, un evento.","metadata":{}},{"cell_type":"code","source":"class elapsed(object):\n    def __init__(self, fld):\n        self.fld = fld\n        self.last = pd.to_datetime(np.nan)\n        self.last_store = 0\n        \n    def get(self, row):\n        if row.Store != self.last_store:\n            self.last = pd.to_datetime(np.nan)\n            self.last_store = row.Store\n        if (row[self.fld]): self.last = row.Date\n        return row.Date-self.last\n\ndef add_elapsed(df, fld, prefix):\n    sh_el = elapsed(fld)\n    df[prefix+fld] = df.apply(sh_el.get, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday_bool\", \"SchoolHoliday\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = joined_train[columns]\ndf_test = joined_test[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para poder observar distintos patrones sobre la demanda en base a las fechas, podemos determinar cuanto tiempo se tiene antes o despues de un feriado escolar. Por ejemplo, puede ser que la gente espera a un feriado para ir a realizar las compras, por lo que se observaría que baja la demanda al rededor del feriado y crece durante el feriado.","metadata":{}},{"cell_type":"code","source":"fld = 'SchoolHoliday'\n\n# SchoolHoliday - Train - Before & After\ndf_train = df_train.sort_values(['Store', 'Date'], ascending=[True, False])\nadd_elapsed(df_train, fld, 'Before')\ndf_train = df_train.sort_values(['Store', 'Date'])\nadd_elapsed(df_train, fld, 'After')\n\n# SchoolHoliday - Test - Before & After\ndf_test = df_test.sort_values(['Store', 'Date'], ascending=[True, False])\nadd_elapsed(df_test, fld, 'Before')\ndf_test = df_test.sort_values(['Store', 'Date'])\nadd_elapsed(df_test, fld, 'After')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se repite para los distintos eventos.","metadata":{}},{"cell_type":"code","source":"fld = 'StateHoliday_bool'\n\n# StateHoliday - Train - Before & After\ndf_train = df_train.sort_values(['Store', 'Date'], ascending=[True, False])\nadd_elapsed(df_train, fld, 'Before')\ndf_train = df_train.sort_values(['Store', 'Date'])\nadd_elapsed(df_train, fld, 'After')\n\n# StateHoliday - Test - Before & After\ndf_test = df_test.sort_values(['Store', 'Date'], ascending=[True, False])\nadd_elapsed(df_test, fld, 'Before')\ndf_test = df_test.sort_values(['Store', 'Date'])\nadd_elapsed(df_test, fld, 'After')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fld = 'Promo'\n\n# Promo - Train - Before & After\ndf_train = df_train.sort_values(['Store', 'Date'], ascending=[True, False])\nadd_elapsed(df_train, fld, 'Before')\ndf_train = df_train.sort_values(['Store', 'Date'])\nadd_elapsed(df_train, fld, 'After')\n\n# Promo - Test - Before & After\ndf_test = df_test.sort_values(['Store', 'Date'], ascending=[True, False])\nadd_elapsed(df_test, fld, 'Before')\ndf_test = df_test.sort_values(['Store', 'Date'])\nadd_elapsed(df_test, fld, 'After')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.set_index(\"Date\")\ndf_test = df_test.set_index(\"Date\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sort_values('Date').head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.sort_values('Date').head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Eliminar los NaT","metadata":{}},{"cell_type":"code","source":"columns = ['SchoolHoliday', 'StateHoliday_bool', 'Promo']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for o in ['Before', 'After']:\n    for p in columns:\n        a = o+p\n        df_train[a] = df_train[a].fillna(pd.Timedelta(0)).dt.days\n        df_test[a] = df_test[a].fillna(pd.Timedelta(0)).dt.days","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sort_values('Date').head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.sort_values('Date').head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.to_csv('df_train_before_bwdfwd.csv')\ndf_test.to_csv('df_test_before_bwdfwd.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Suavizado de Columnas Temporales","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['SchoolHoliday', 'StateHoliday_bool', 'Promo']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SKIP = './'\ndf_train = pd.read_csv(SKIP + 'df_train_before_bwdfwd.csv')\ndf_test = pd.read_csv(SKIP + 'df_test_before_bwdfwd.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train[['Store']+columns]['Store'] == 1].sort_values('Date').head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Función para reducir el uso de memoria","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bwd_train = df_train[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()\nbwd_test = df_test[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()\n\nbwd_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bwd_train.drop('Store',1,inplace=True)\nbwd_train.reset_index(inplace=True)\n#bwd_test.drop('Store',1,inplace=True)\nbwd_test.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bwd_train = reduce_mem_usage(bwd_train)\nbwd_test = reduce_mem_usage(bwd_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ordena al reves\nfwd_train = df_train[['Store']+columns].sort_index(ascending=False).groupby(\"Store\").rolling(7, min_periods=1).sum()\nfwd_test = df_test[['Store']+columns].sort_index(ascending=False).groupby(\"Store\").rolling(7, min_periods=1).sum()\nfwd_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fwd_train.drop('Store',1,inplace=True)\nfwd_train.reset_index(inplace=True)\n#fwd_test.drop('Store',1,inplace=True)\nfwd_test.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fwd_train = reduce_mem_usage(fwd_train)\nfwd_test = reduce_mem_usage(fwd_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train.reset_index(inplace=True)\n#df_test.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.merge(bwd_train, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\ndf_train = df_train.merge(fwd_train, 'left', ['Date', 'Store'], suffixes=['', '_fw'])\ndf_test = df_test.merge(bwd_test, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\ndf_test = df_test.merge(fwd_test, 'left', ['Date', 'Store'], suffixes=['', '_fw'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.drop(columns,1,inplace=True)\ndf_test.drop(columns,1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"Date\"] = pd.to_datetime(df_train.Date)\ndf_test[\"Date\"] = pd.to_datetime(df_test.Date)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,3))\ndf_train[df_train['Store'] == 280]['BeforeSchoolHoliday'].plot()\ndf_train[df_train['Store'] == 280]['BeforeStateHoliday_bool'].plot()\ndf_train[df_train['Store'] == 280]['BeforePromo'].plot()\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(20,3))\ndf_train[df_train['Store'] == 280]['AfterSchoolHoliday'].plot()\ndf_train[df_train['Store'] == 280]['AfterStateHoliday_bool'].plot()\ndf_train[df_train['Store'] == 280]['AfterPromo'].plot()\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joined_train = join_df(joined_train, df_train, ['Store', 'Date'])\njoined_test = join_df(joined_test, df_test, ['Store', 'Date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joined_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joined_train.to_feather('all_preprocessed_train.fth')\njoined_test.to_feather('all_preprocessed_test.fth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalize & Encode","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SKIP = './'\n\ndf_train = pd.read_feather(SKIP + 'all_preprocessed_train.fth')\ndf_test = pd.read_feather(SKIP + 'all_preprocessed_test.fth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificación de que se hicieron bien las cosas en testa también\nset(df_train.columns) - set(df_test.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df_train['CompetitionDistance'].isna()).sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variables Categoricas","metadata":{}},{"cell_type":"code","source":"cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', \n            'CompetitionMonthsOpen', 'Promo2Weeks', \n            'StoreType', 'Assortment', 'PromoInterval', \n            'CompetitionOpenSinceYear', 'Promo2SinceYear', \n            'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', \n            'StateHoliday_bool_fw', 'StateHoliday_bool_bw', 'SchoolHoliday_fw', \n            'SchoolHoliday_bw']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DataFrameSummary(df_train[cat_vars]).summary().loc[['uniques', 'types', 'missing']].T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variables Continuas","metadata":{}},{"cell_type":"code","source":"contin_vars = ['CompetitionDistance', \n               'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', \n               'Precipitationmm', 'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', \n               'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', \n               'trend_DE', 'AfterStateHoliday_bool', 'BeforeStateHoliday_bool', \n               'Promo', 'SchoolHoliday', 'StateHoliday_bool']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DataFrameSummary(df_train[contin_vars]).summary().loc[['uniques', 'types', 'missing']].T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalización en continuas y LabelEncode en Categoricas","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn_pandas import DataFrameMapper","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_maps = [(o, LabelEncoder()) for o in cat_vars]\ncontin_maps = [([o], StandardScaler()) for o in contin_vars]\n\nmapper_cat = DataFrameMapper(cat_maps)\n_ = mapper_cat.fit(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assortment es la novena posición en cat_maps\nN = 10\nprint(list(zip(df_train['Assortment'].values[:N], mapper_cat.transform(df_train)[:,9][:N])))\nprint(list(zip(df_train['Events'].values[:N], mapper_cat.transform(df_train)[:,15][:N])))\nprint(list(zip(df_train['Year'].values[:N], mapper_cat.transform(df_train)[:,2][:N])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapper_cont = DataFrameMapper(contin_maps)\n_ = mapper_cont.fit(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 10\nprint(df_train['CompetitionDistance'].values[:N])\nprint(mapper_cont.transform(df_train)[:, 0][:N])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# La hacemos con todas ahora\ndf_train[cat_vars] = mapper_cat.transform(df_train)\ndf_test[cat_vars] = mapper_cat.transform(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[contin_vars] = mapper_cont.transform(df_train)\ndf_test[contin_vars] = mapper_cont.transform(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DataFrameSummary(df_train[cat_vars]).summary().loc[['uniques', 'types']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Processing","metadata":{}},{"cell_type":"code","source":"# Ya que lo que me interesa y lo que busco estimar son las sales, me quedo con las sales que no sean 0\ndf_sales = df_train[df_train.Sales!=0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.reset_index(inplace=True)\ndf_sales.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sales.to_feather('train_normalized_data.fth')\ndf_test.to_feather('test_normalized_data.fth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baselines\nBusco un punto de referencia para comparar los resultados","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime\nfrom pandas_summary import DataFrameSummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SKIP = './'\ndf = pd.read_feather(SKIP + 'train_normalized_data.fth')\ndf_test = pd.read_feather(SKIP + 'test_normalized_data.fth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df[df.Date < datetime.datetime(2015, 7, 1)]  \ndf_val = df[df.Date >= datetime.datetime(2015, 7, 1)]\nlen(df_train)/len(df), len(df_val)/len(df), len(df), len(df_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Métrica de la competencia\nLa métrica de la competencia es el Root Mean Square Error (RMSE). EL RMSE se define como:\n\n$$RMSE\\;=\\; \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\frac{\\widehat{y_i} - y_i}{y_i})^2}$$\n\nEl concepto detras de utilizar esta métrica es obtener una desviación de las predicciones basándose en los datos reales. Matemáticamente, se trata de un error cuadratico, por lo que los valores pequeños se benefician y los grandes se castigan. A su vez se tiene que es un promedio sobre toda la muestra, por lo que se estima un panorama general del resultado. Igualmente, debido a que los errores grandes se castigan, el RMSE es fácilmente influenciado por *outliers*, valores que se desvían considerablemente del resto.\nEn términos generales, el RMSE nos dice que tan lejos se encuentran las observaciones reales de la línea de mejor ajuste creada.","metadata":{}},{"cell_type":"code","source":"#defino la metrica de la competencia\ndef get_metric(sales, sales_):\n    return np.sqrt((((sales - sales_)/sales)**2).sum()/len(sales))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalizo las Sales\nmax_sales = df_train['Sales'].max()\ndf.loc[:, 'Sales_norm'] = df['Sales'].values/max_sales\n\ndf_train.loc[:, 'Sales_norm'] = df_train['Sales'].values/max_sales\ndf_val.loc[:, 'Sales_norm'] = df_val['Sales'].values/max_sales","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculo la metrica usando las medias como 'predicción'","metadata":{}},{"cell_type":"code","source":"print('Train:')\nprint(get_metric(df_train['Sales_norm'], \n                 df_train['Sales_norm'].mean()))\nprint('Val:')\nget_metric(df_val['Sales_norm'], \n           df_train['Sales_norm'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se observa que las medias no son muy representativas como predicción.","metadata":{}},{"cell_type":"code","source":"# Funcion para observar las medias de cada columnia# Media por store\ndef get_mean_by_column(column, sales_str):\n    group_means_dict = {}\n    group_mean_list = []\n    for col_value, group_df in df_train.groupby(column):\n        group_mean =  group_df[group_df[sales_str] > 0][sales_str].mean()\n        group_means_dict[col_value] = group_mean\n        group_mean_list.append(group_mean)\n    print('Train:', get_metric(df_train[sales_str], \n                               df_train[column].apply(group_means_dict.get)))\n    print('Val:', get_metric(df_val[sales_str], \n                             df_val[column].apply(group_means_dict.get)))\n    return group_means_dict, group_mean_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Media por store\n_ = get_mean_by_column('Store', 'Sales_norm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Media por dia de la semana\n_ = get_mean_by_column('DayOfWeek', 'Sales_norm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Media por numera de semana (1-52)\n_ = get_mean_by_column('Week', 'Sales_norm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Desarrollo del Modelo","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense, BatchNormalization, Activation, LeakyReLU, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import callbacks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    return K.sqrt(K.mean(K.square((y_true - y_pred)/y_true)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Definición del modelo\nEl modelo se define utilizando un metodo para ordenar las entradas llamado 'entity embeddings'. Los entity embedings ordenan las categorias en base a que tan similares son los valores que se tienen. Es decir, si dos categorias tienen compartamientos similares, entonces estas se agrupan más cercanas. Este agrupamiento convierte las categorias en variables continuas, de tal manera que se simplifique la modelización, y a su vez, mejorando los resultados.","metadata":{}},{"cell_type":"code","source":"def get_embedings_NN(X_columns, hidden_units = 20, activation = 'relu'):\n    embed_outs = []\n    inputs = []\n    for i, col in enumerate(X_columns):\n        inp = Input(shape=(1,), name=f\"{col}_input\")\n        inputs.append(inp)\n        if col in embed_outs_dict:\n            embed_out = Embedding(len(np.unique(X_train[i])), embed_outs_dict[col], name=f\"{col}_embedding\", mask_zero=False)(inp)\n            out = Flatten(name=f\"{col}_flatten\")(embed_out)\n            embed_outs.append(out)\n        else:\n            embed_outs.append(inp)\n        \n    if len(X_columns)>1:\n        concat_out = Concatenate()(embed_outs)\n        dense_out = Dense(hidden_units, activation=activation)(concat_out)\n    else:\n        dense_out = Dense(hidden_units, activation=activation)(out)\n    out = Dense(1)(dense_out)\n    model = Model(inputs, out)\n    model.compile(optimizers.Adam(lr=0.0001), loss='mse', metrics=[rmspe, 'mse'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_outs_dict = {'Store': 2, 'DayOfWeek': 2} #,  'Week': 2, 'Month': 2}\nX_columns = list(embed_outs_dict.keys())# + ['BeforeStateHoliday_bool', 'Max_TemperatureC'] # ['Precipitationmm']\n\nif final_train:\n    X_train = np.hsplit(df[X_columns].values, len(X_columns))\n    y_train = df['Sales_norm']\nelse:\n    X_train = np.hsplit(df_train[X_columns].values, len(X_columns))\n    y_train = df_train['Sales_norm']\n    \nX_val = np.hsplit(df_val[X_columns].values, len(X_columns))\nX_test = np.hsplit(df_test[X_columns].values, len(X_columns))\n\ny_val = df_val['Sales_norm']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_embedings_NN(X_columns)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nmodel.compile(optimizers.Adam(lr=0.001), loss='mse', metrics=[rmspe, 'mse'])\ncbs = [callbacks.ReduceLROnPlateau(monitor='val_rmspe', mode='min', verbose=1, patience=2)]\nmodel.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), callbacks=cbs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualización de Embeddings","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_embed(layer_name, cat_names):\n    Y = model.get_layer(layer_name).get_weights()[0]\n    print(Y.shape)\n    plt.figure(figsize=(8,8))\n    plt.scatter(-Y[:, 0], -Y[:, 1])\n    for i, txt in enumerate(cat_names):\n        plt.annotate(txt, (-Y[i, 0],-Y[i, 1]), xytext = (-5, 8), textcoords = 'offset points')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_embed('DayOfWeek_embedding', ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat','Sun'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_embed('Store_embedding', list(range(1115)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrica en base a las predicciones","metadata":{}},{"cell_type":"code","source":"train_predictions = model.predict(X_train)*max_sales\nget_metric(df_train['Sales'].values, train_predictions.reshape(-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = model.predict(X_test)*max_sales\ntest_predictions[df_test['Open'] == 0] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline submission para comparar a futuro","metadata":{}},{"cell_type":"code","source":"sample_csv = pd.read_csv('../input/rossman-data/sample_submission.csv')\nsample_csv['Sales'] = test_predictions\nsample_csv.head()\n\nsample_csv.to_csv(f'submision_baseline_{\"-\".join(X_columns)}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Nuevo Modelo","metadata":{}},{"cell_type":"markdown","source":"## Asignación de dimensión de embeddings\nLe asginamos una mayor dimensión a las categorias que consideramos mas complejas.","metadata":{}},{"cell_type":"code","source":"cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', \n            'CompetitionMonthsOpen', 'Promo2Weeks', \n            'StoreType', 'Assortment', 'PromoInterval', \n            'CompetitionOpenSinceYear', 'Promo2SinceYear', \n            'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', \n            'StateHoliday_bool_fw', 'StateHoliday_bool_bw', 'SchoolHoliday_fw', \n            'SchoolHoliday_bw']\n\ncat_var_dict = {'Store': 50, 'DayOfWeek': 2, 'Year': 2, 'Month': 2,\n'Day': 10, 'StateHoliday': 2, 'CompetitionMonthsOpen': 2,\n'Promo2Weeks': 1, 'StoreType': 2, 'Assortment': 3, 'PromoInterval': 3,\n'CompetitionOpenSinceYear': 4, 'Promo2SinceYear': 4, 'State': 6,\n'Week': 25, 'Events': 4, 'Promo_fw': 1,\n'Promo_bw': 1, 'StateHoliday_bool_fw': 1,\n'StateHoliday_bool_bw': 1, 'SchoolHoliday_fw': 1,\n'SchoolHoliday_bw': 1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contin_vars = ['CompetitionDistance', \n               'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', \n               'Precipitationmm', 'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', \n               'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', \n               'trend_DE', 'AfterStateHoliday_bool', 'BeforeStateHoliday_bool', \n               'Promo', 'SchoolHoliday', 'StateHoliday_bool']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uniques = DataFrameSummary(df[cat_vars]).summary().loc[['uniques']]\nfor v in cat_vars:\n    uniques_ = df[v].unique()\n    uniques_.sort()\n    print(v, cat_var_dict[v], len(uniques_), uniques_)\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Definición de modelo","metadata":{}},{"cell_type":"code","source":"add_customers = True\nlog_output = True\noutput_activation = 'softmax'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cat_vars_model(cat_vars, uniques, cat_var_dict):\n    cat_vars_embed_outs = []\n    cat_var_inputs = []\n    for cat_var in cat_vars:\n        cat_var_in = Input(shape=(1,), name=f\"{cat_var}_input\")\n        cat_var_inputs.append(cat_var_in)\n        embed_out = Embedding(uniques[cat_var][0], cat_var_dict[cat_var], name=f'{cat_var}_Embed')(cat_var_in)\n        flatten_out = Flatten(name=f\"{cat_var}_flat\")(embed_out)\n        cat_vars_embed_outs.append(flatten_out)\n    return cat_var_inputs, cat_vars_embed_outs\n\ndef get_cont_vars_input(contin_vars, dense_layer=False):\n    cont_vars_inputs = []\n    cont_vars_outputs = []\n    for cont_var in contin_vars:\n        cont_var_in = Input(shape=(1,), name=f\"{cont_var}_input\")\n        cont_vars_inputs.append(cont_var_in)\n        if dense_layer:\n            cont_var_out = Dense(1, name=f\"{cont_var}_input\", activation = 'linear')(cont_var_in)\n            cont_vars_outputs.append(cont_var_out)\n        else:\n            cont_vars_outputs.append(cont_var_in)\n    return cont_vars_inputs, cont_vars_outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_var_inputs, cat_vars_embed_outs = get_cat_vars_model(cat_vars, uniques, cat_var_dict)\ncont_vars_inputs,  cont_vars_outs= get_cont_vars_input(contin_vars)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_vars_embed_outs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_var_inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_vars_inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_vars_outs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_hidden_units = 1000\nsecond_hidden_units = 500\nl2_lambda = 1e-3\nmerged = Concatenate(name='All_Concatenate')(cat_vars_embed_outs + cont_vars_inputs)\nx = Dense(first_hidden_units, kernel_initializer=\"uniform\", kernel_regularizer=l2(l2_lambda))(merged)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = LeakyReLU()(x)\nx = Dense(second_hidden_units, kernel_initializer=\"uniform\", kernel_regularizer=l2(l2_lambda))(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = LeakyReLU()(x)\n\noutput_1 = Dense(1, name='Sales', activation=output_activation)(x)\noutput_2 = Dense(1, name='Customers', activation=output_activation)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if add_customers:\n    model = Model(cat_var_inputs + cont_vars_inputs, [output_1, output_2])\nelse: \n    model = Model(cat_var_inputs + cont_vars_inputs, [output_1])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('initial_weights.hdf5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Cantidad en val: {len(df_val)}, porcentaje: {len(df_train)/(len(df_train) + len(df_val))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_vars = cat_vars + contin_vars\nX_train = np.hsplit(df_train[all_vars].values, len(all_vars))\nX_val = np.hsplit(df_val[all_vars].values, len(all_vars))\nX_test = np.hsplit(df_test[all_vars].values, len(all_vars))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if add_customers:\n    y_out_columns = ['Sales', 'Customers']\nelse:\n    y_out_columns = ['Sales_store']\n    \nif log_output:\n    # Escala logaritmica\n    max_log_y = np.max(np.log(df[y_out_columns])).values\n    y_train = np.log(df_train[y_out_columns].values)/max_log_y\n    y_val = np.log(df_val[y_out_columns].values)/max_log_y\nelse:\n    # Normalización\n    y_mean = df_train[y_out_columns].mean().values\n    y_std = df_train[y_out_columns].std().values\n    y_train = (df_train[y_out_columns].values - y_mean)/y_std\n    y_val = (df_val[y_out_columns].values - y_mean)/y_std\n    y_max = df_train[y_out_columns].max().values\n    y_train = df_train[y_out_columns].values/y_max\n    y_val = df_val[y_out_columns].values/y_max\ny_train = np.hsplit(y_train, y_train.shape[1])\ny_val = np.hsplit(y_val, y_val.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 0.001\nmodel.compile(optimizer=Adam(lr=lr), metrics=['mse', rmspe], loss='mse')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if add_customers:\n    checkpoint = ModelCheckpoint('bestmodel.hdf5', monitor='val_Sales_mse', verbose=1, save_best_only=True)\nelse:\n    checkpoint = ModelCheckpoint('bestmodel.hdf5', monitor='val_loss', verbose=1, save_best_only=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 20\nbatch_size = 128\nhistory = model.fit(X_train, y_train, validation_data=(X_val, y_val),  epochs=epochs, batch_size=batch_size, callbacks=[checkpoint], verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['val_loss'])\nplt.show()\nplt.plot(history.history['loss'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"model.evaluate(X_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('bestmodel.hdf5')\nmodel.evaluate(X_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if log_output:\n    if add_customers:\n        y_pred = np.exp(model.predict(X_val, verbose=1)[0][:, 0]*max_log_y[0])\n        y_pred_test = np.exp(model.predict(X_test, verbose=1)[0][:, 0]*max_log_y[0])\n    else:\n        y_pred = np.exp(model.predict(X_val, verbose=1)*max_log_y)[:,0]\n        y_pred_test = np.exp(model.predict(X_test, verbose=1)*max_log_y)[:,0]\nelse:\n    if add_customers:\n        y_pred = (model.predict(X_val, verbose=1)[0]*y_std[0] + y_mean[0])[:,0]\n        y_pred_test = (model.predict(X_test, verbose=1)[0]*y_std[0] + y_mean[0])[:,0]\n    else:\n#         y_pred = model.predict(X_val, verbose=1)[:,0]*y_std + y_mean\n#         y_pred_test = model.predict(X_test, verbose=1)[:,0]*y_std + y_mean\n        y_pred = model.predict(X_val, verbose=1)[:,0]*y_max\n        y_pred_test = model.predict(X_test, verbose=1)[:,0]*y_max\ny_pred_test[df_test['Open'] == 0] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_csv = pd.read_csv('../input/rossman-data/sample_submission.csv')\nsample_csv['Sales'] = y_pred_test\nsample_csv.head()\n\nsample_csv.to_csv(f'submision_{add_customers}-{log_output}-{output_activation}-{l2_lambda}-{first_hidden_units}-{epochs}-{batch_size}-{lr}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}